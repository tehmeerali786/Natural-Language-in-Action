{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 9 - NLPIA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2WPm-eqlabE"
      },
      "source": [
        "import glob\r\n",
        "import os\r\n",
        "\r\n",
        "from random import shuffle\r\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt570kyNmPug",
        "outputId": "d8081db2-db5a-41f0-9f34-7323ffe0c996"
      },
      "source": [
        "!unzip /content/pos2.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/pos2.zip\n",
            "  inflating: pos2/0_9.txt            \n",
            "  inflating: pos2/1_7.txt            \n",
            "  inflating: pos2/10_9.txt           \n",
            "  inflating: pos2/100_7.txt          \n",
            "  inflating: pos2/101_8.txt          \n",
            "  inflating: pos2/102_10.txt         \n",
            "  inflating: pos2/103_7.txt          \n",
            "  inflating: pos2/104_10.txt         \n",
            "  inflating: pos2/105_7.txt          \n",
            "  inflating: pos2/106_10.txt         \n",
            "  inflating: pos2/107_10.txt         \n",
            "  inflating: pos2/108_10.txt         \n",
            "  inflating: pos2/109_10.txt         \n",
            "  inflating: pos2/11_9.txt           \n",
            "  inflating: pos2/110_10.txt         \n",
            "  inflating: pos2/111_10.txt         \n",
            "  inflating: pos2/112_10.txt         \n",
            "  inflating: pos2/113_10.txt         \n",
            "  inflating: pos2/114_10.txt         \n",
            "  inflating: pos2/115_10.txt         \n",
            "  inflating: pos2/116_10.txt         \n",
            "  inflating: pos2/117_10.txt         \n",
            "  inflating: pos2/118_8.txt          \n",
            "  inflating: pos2/119_10.txt         \n",
            "  inflating: pos2/12_9.txt           \n",
            "  inflating: pos2/120_8.txt          \n",
            "  inflating: pos2/121_10.txt         \n",
            "  inflating: pos2/122_9.txt          \n",
            "  inflating: pos2/123_10.txt         \n",
            "  inflating: pos2/124_10.txt         \n",
            "  inflating: pos2/125_7.txt          \n",
            "  inflating: pos2/126_10.txt         \n",
            "  inflating: pos2/127_7.txt          \n",
            "  inflating: pos2/128_7.txt          \n",
            "  inflating: pos2/129_9.txt          \n",
            "  inflating: pos2/13_7.txt           \n",
            "  inflating: pos2/130_9.txt          \n",
            "  inflating: pos2/131_10.txt         \n",
            "  inflating: pos2/132_9.txt          \n",
            "  inflating: pos2/133_10.txt         \n",
            "  inflating: pos2/134_10.txt         \n",
            "  inflating: pos2/135_7.txt          \n",
            "  inflating: pos2/136_10.txt         \n",
            "  inflating: pos2/137_7.txt          \n",
            "  inflating: pos2/138_7.txt          \n",
            "  inflating: pos2/139_10.txt         \n",
            "  inflating: pos2/14_10.txt          \n",
            "  inflating: pos2/140_8.txt          \n",
            "  inflating: pos2/141_9.txt          \n",
            "  inflating: pos2/142_8.txt          \n",
            "  inflating: pos2/143_7.txt          \n",
            "  inflating: pos2/144_8.txt          \n",
            "  inflating: pos2/145_10.txt         \n",
            "  inflating: pos2/146_10.txt         \n",
            "  inflating: pos2/147_9.txt          \n",
            "  inflating: pos2/148_9.txt          \n",
            "  inflating: pos2/149_10.txt         \n",
            "  inflating: pos2/15_7.txt           \n",
            "  inflating: pos2/150_8.txt          \n",
            "  inflating: pos2/151_10.txt         \n",
            "  inflating: pos2/152_9.txt          \n",
            "  inflating: pos2/153_10.txt         \n",
            "  inflating: pos2/154_8.txt          \n",
            "  inflating: pos2/155_10.txt         \n",
            "  inflating: pos2/156_8.txt          \n",
            "  inflating: pos2/157_9.txt          \n",
            "  inflating: pos2/158_10.txt         \n",
            "  inflating: pos2/159_10.txt         \n",
            "  inflating: pos2/16_7.txt           \n",
            "  inflating: pos2/160_9.txt          \n",
            "  inflating: pos2/161_8.txt          \n",
            "  inflating: pos2/162_8.txt          \n",
            "  inflating: pos2/163_10.txt         \n",
            "  inflating: pos2/164_10.txt         \n",
            "  inflating: pos2/165_7.txt          \n",
            "  inflating: pos2/166_7.txt          \n",
            "  inflating: pos2/167_7.txt          \n",
            "  inflating: pos2/168_9.txt          \n",
            "  inflating: pos2/169_8.txt          \n",
            "  inflating: pos2/17_9.txt           \n",
            "  inflating: pos2/170_10.txt         \n",
            "  inflating: pos2/171_8.txt          \n",
            "  inflating: pos2/172_10.txt         \n",
            "  inflating: pos2/173_7.txt          \n",
            "  inflating: pos2/174_7.txt          \n",
            "  inflating: pos2/175_7.txt          \n",
            "  inflating: pos2/176_7.txt          \n",
            "  inflating: pos2/177_9.txt          \n",
            "  inflating: pos2/178_7.txt          \n",
            "  inflating: pos2/179_8.txt          \n",
            "  inflating: pos2/18_7.txt           \n",
            "  inflating: pos2/180_9.txt          \n",
            "  inflating: pos2/181_10.txt         \n",
            "  inflating: pos2/182_10.txt         \n",
            "  inflating: pos2/183_8.txt          \n",
            "  inflating: pos2/184_8.txt          \n",
            "  inflating: pos2/185_9.txt          \n",
            "  inflating: pos2/186_8.txt          \n",
            "  inflating: pos2/187_8.txt          \n",
            "  inflating: pos2/188_7.txt          \n",
            "  inflating: pos2/189_9.txt          \n",
            "  inflating: pos2/19_10.txt          \n",
            "  inflating: pos2/190_10.txt         \n",
            "  inflating: pos2/191_9.txt          \n",
            "  inflating: pos2/192_9.txt          \n",
            "  inflating: pos2/193_7.txt          \n",
            "  inflating: pos2/194_8.txt          \n",
            "  inflating: pos2/195_8.txt          \n",
            "  inflating: pos2/196_9.txt          \n",
            "  inflating: pos2/197_9.txt          \n",
            "  inflating: pos2/198_8.txt          \n",
            "  inflating: pos2/199_10.txt         \n",
            "  inflating: pos2/2_9.txt            \n",
            "  inflating: pos2/20_9.txt           \n",
            "  inflating: pos2/200_10.txt         \n",
            "  inflating: pos2/201_10.txt         \n",
            "  inflating: pos2/202_10.txt         \n",
            "  inflating: pos2/203_7.txt          \n",
            "  inflating: pos2/204_10.txt         \n",
            "  inflating: pos2/205_8.txt          \n",
            "  inflating: pos2/206_10.txt         \n",
            "  inflating: pos2/207_8.txt          \n",
            "  inflating: pos2/208_9.txt          \n",
            "  inflating: pos2/209_8.txt          \n",
            "  inflating: pos2/21_7.txt           \n",
            "  inflating: pos2/210_10.txt         \n",
            "  inflating: pos2/211_9.txt          \n",
            "  inflating: pos2/212_9.txt          \n",
            "  inflating: pos2/213_9.txt          \n",
            "  inflating: pos2/214_7.txt          \n",
            "  inflating: pos2/215_8.txt          \n",
            "  inflating: pos2/216_8.txt          \n",
            "  inflating: pos2/217_8.txt          \n",
            "  inflating: pos2/218_9.txt          \n",
            "  inflating: pos2/219_8.txt          \n",
            "  inflating: pos2/22_8.txt           \n",
            "  inflating: pos2/220_10.txt         \n",
            "  inflating: pos2/221_9.txt          \n",
            "  inflating: pos2/222_10.txt         \n",
            "  inflating: pos2/223_9.txt          \n",
            "  inflating: pos2/224_10.txt         \n",
            "  inflating: pos2/225_9.txt          \n",
            "  inflating: pos2/226_10.txt         \n",
            "  inflating: pos2/227_10.txt         \n",
            "  inflating: pos2/228_7.txt          \n",
            "  inflating: pos2/229_10.txt         \n",
            "  inflating: pos2/23_7.txt           \n",
            "  inflating: pos2/230_9.txt          \n",
            "  inflating: pos2/231_10.txt         \n",
            "  inflating: pos2/232_10.txt         \n",
            "  inflating: pos2/233_7.txt          \n",
            "  inflating: pos2/234_10.txt         \n",
            "  inflating: pos2/235_10.txt         \n",
            "  inflating: pos2/236_9.txt          \n",
            "  inflating: pos2/237_10.txt         \n",
            "  inflating: pos2/238_10.txt         \n",
            "  inflating: pos2/239_7.txt          \n",
            "  inflating: pos2/24_8.txt           \n",
            "  inflating: pos2/240_10.txt         \n",
            "  inflating: pos2/241_8.txt          \n",
            "  inflating: pos2/242_8.txt          \n",
            "  inflating: pos2/243_10.txt         \n",
            "  inflating: pos2/244_10.txt         \n",
            "  inflating: pos2/245_9.txt          \n",
            "  inflating: pos2/246_7.txt          \n",
            "  inflating: pos2/247_10.txt         \n",
            "  inflating: pos2/248_10.txt         \n",
            "  inflating: pos2/249_10.txt         \n",
            "  inflating: pos2/25_7.txt           \n",
            "  inflating: pos2/250_7.txt          \n",
            "  inflating: pos2/251_10.txt         \n",
            "  inflating: pos2/252_9.txt          \n",
            "  inflating: pos2/253_7.txt          \n",
            "  inflating: pos2/254_8.txt          \n",
            "  inflating: pos2/255_10.txt         \n",
            "  inflating: pos2/256_9.txt          \n",
            "  inflating: pos2/257_7.txt          \n",
            "  inflating: pos2/258_7.txt          \n",
            "  inflating: pos2/259_8.txt          \n",
            "  inflating: pos2/26_9.txt           \n",
            "  inflating: pos2/260_7.txt          \n",
            "  inflating: pos2/261_8.txt          \n",
            "  inflating: pos2/262_8.txt          \n",
            "  inflating: pos2/263_9.txt          \n",
            "  inflating: pos2/264_7.txt          \n",
            "  inflating: pos2/265_7.txt          \n",
            "  inflating: pos2/266_7.txt          \n",
            "  inflating: pos2/267_7.txt          \n",
            "  inflating: pos2/268_8.txt          \n",
            "  inflating: pos2/269_8.txt          \n",
            "  inflating: pos2/27_10.txt          \n",
            "  inflating: pos2/270_10.txt         \n",
            "  inflating: pos2/271_10.txt         \n",
            "  inflating: pos2/272_10.txt         \n",
            "  inflating: pos2/273_9.txt          \n",
            "  inflating: pos2/274_7.txt          \n",
            "  inflating: pos2/275_10.txt         \n",
            "  inflating: pos2/276_10.txt         \n",
            "  inflating: pos2/277_8.txt          \n",
            "  inflating: pos2/278_9.txt          \n",
            "  inflating: pos2/279_9.txt          \n",
            "  inflating: pos2/28_10.txt          \n",
            "  inflating: pos2/280_8.txt          \n",
            "  inflating: pos2/281_10.txt         \n",
            "  inflating: pos2/282_9.txt          \n",
            "  inflating: pos2/283_8.txt          \n",
            "  inflating: pos2/284_10.txt         \n",
            "  inflating: pos2/285_10.txt         \n",
            "  inflating: pos2/286_10.txt         \n",
            "  inflating: pos2/287_9.txt          \n",
            "  inflating: pos2/288_10.txt         \n",
            "  inflating: pos2/289_10.txt         \n",
            "  inflating: pos2/29_10.txt          \n",
            "  inflating: pos2/290_9.txt          \n",
            "  inflating: pos2/291_10.txt         \n",
            "  inflating: pos2/292_10.txt         \n",
            "  inflating: pos2/293_7.txt          \n",
            "  inflating: pos2/294_10.txt         \n",
            "  inflating: pos2/295_10.txt         \n",
            "  inflating: pos2/296_10.txt         \n",
            "  inflating: pos2/297_10.txt         \n",
            "  inflating: pos2/298_8.txt          \n",
            "  inflating: pos2/299_10.txt         \n",
            "  inflating: pos2/3_10.txt           \n",
            "  inflating: pos2/30_7.txt           \n",
            "  inflating: pos2/300_9.txt          \n",
            "  inflating: pos2/301_10.txt         \n",
            "  inflating: pos2/302_10.txt         \n",
            "  inflating: pos2/303_10.txt         \n",
            "  inflating: pos2/304_10.txt         \n",
            "  inflating: pos2/305_8.txt          \n",
            "  inflating: pos2/306_10.txt         \n",
            "  inflating: pos2/307_8.txt          \n",
            "  inflating: pos2/308_8.txt          \n",
            "  inflating: pos2/309_9.txt          \n",
            "  inflating: pos2/31_8.txt           \n",
            "  inflating: pos2/310_7.txt          \n",
            "  inflating: pos2/311_9.txt          \n",
            "  inflating: pos2/312_10.txt         \n",
            "  inflating: pos2/313_10.txt         \n",
            "  inflating: pos2/314_10.txt         \n",
            "  inflating: pos2/315_10.txt         \n",
            "  inflating: pos2/316_10.txt         \n",
            "  inflating: pos2/317_10.txt         \n",
            "  inflating: pos2/318_10.txt         \n",
            "  inflating: pos2/319_9.txt          \n",
            "  inflating: pos2/32_10.txt          \n",
            "  inflating: pos2/320_8.txt          \n",
            "  inflating: pos2/321_10.txt         \n",
            "  inflating: pos2/322_10.txt         \n",
            "  inflating: pos2/323_10.txt         \n",
            "  inflating: pos2/324_8.txt          \n",
            "  inflating: pos2/325_9.txt          \n",
            "  inflating: pos2/326_10.txt         \n",
            "  inflating: pos2/327_8.txt          \n",
            "  inflating: pos2/328_10.txt         \n",
            "  inflating: pos2/329_10.txt         \n",
            "  inflating: pos2/33_7.txt           \n",
            "  inflating: pos2/330_10.txt         \n",
            "  inflating: pos2/331_10.txt         \n",
            "  inflating: pos2/332_10.txt         \n",
            "  inflating: pos2/333_10.txt         \n",
            "  inflating: pos2/334_10.txt         \n",
            "  inflating: pos2/335_10.txt         \n",
            "  inflating: pos2/336_10.txt         \n",
            "  inflating: pos2/337_9.txt          \n",
            "  inflating: pos2/338_10.txt         \n",
            "  inflating: pos2/339_10.txt         \n",
            "  inflating: pos2/34_8.txt           \n",
            "  inflating: pos2/340_10.txt         \n",
            "  inflating: pos2/341_10.txt         \n",
            "  inflating: pos2/342_10.txt         \n",
            "  inflating: pos2/343_10.txt         \n",
            "  inflating: pos2/344_8.txt          \n",
            "  inflating: pos2/345_7.txt          \n",
            "  inflating: pos2/346_10.txt         \n",
            "  inflating: pos2/347_10.txt         \n",
            "  inflating: pos2/348_7.txt          \n",
            "  inflating: pos2/349_10.txt         \n",
            "  inflating: pos2/35_8.txt           \n",
            "  inflating: pos2/350_9.txt          \n",
            "  inflating: pos2/351_10.txt         \n",
            "  inflating: pos2/352_10.txt         \n",
            "  inflating: pos2/353_9.txt          \n",
            "  inflating: pos2/354_9.txt          \n",
            "  inflating: pos2/355_9.txt          \n",
            "  inflating: pos2/356_10.txt         \n",
            "  inflating: pos2/357_10.txt         \n",
            "  inflating: pos2/358_10.txt         \n",
            "  inflating: pos2/359_8.txt          \n",
            "  inflating: pos2/36_10.txt          \n",
            "  inflating: pos2/360_10.txt         \n",
            "  inflating: pos2/361_10.txt         \n",
            "  inflating: pos2/362_10.txt         \n",
            "  inflating: pos2/363_10.txt         \n",
            "  inflating: pos2/364_10.txt         \n",
            "  inflating: pos2/365_10.txt         \n",
            "  inflating: pos2/366_9.txt          \n",
            "  inflating: pos2/367_10.txt         \n",
            "  inflating: pos2/368_10.txt         \n",
            "  inflating: pos2/369_10.txt         \n",
            "  inflating: pos2/37_9.txt           \n",
            "  inflating: pos2/370_10.txt         \n",
            "  inflating: pos2/371_9.txt          \n",
            "  inflating: pos2/372_10.txt         \n",
            "  inflating: pos2/373_10.txt         \n",
            "  inflating: pos2/374_10.txt         \n",
            "  inflating: pos2/375_9.txt          \n",
            "  inflating: pos2/376_10.txt         \n",
            "  inflating: pos2/377_7.txt          \n",
            "  inflating: pos2/378_8.txt          \n",
            "  inflating: pos2/379_10.txt         \n",
            "  inflating: pos2/38_10.txt          \n",
            "  inflating: pos2/380_10.txt         \n",
            "  inflating: pos2/381_10.txt         \n",
            "  inflating: pos2/382_10.txt         \n",
            "  inflating: pos2/383_10.txt         \n",
            "  inflating: pos2/384_8.txt          \n",
            "  inflating: pos2/385_10.txt         \n",
            "  inflating: pos2/386_7.txt          \n",
            "  inflating: pos2/387_8.txt          \n",
            "  inflating: pos2/388_8.txt          \n",
            "  inflating: pos2/389_10.txt         \n",
            "  inflating: pos2/39_9.txt           \n",
            "  inflating: pos2/390_10.txt         \n",
            "  inflating: pos2/391_8.txt          \n",
            "  inflating: pos2/392_9.txt          \n",
            "  inflating: pos2/393_8.txt          \n",
            "  inflating: pos2/394_8.txt          \n",
            "  inflating: pos2/395_10.txt         \n",
            "  inflating: pos2/396_8.txt          \n",
            "  inflating: pos2/397_9.txt          \n",
            "  inflating: pos2/398_10.txt         \n",
            "  inflating: pos2/399_9.txt          \n",
            "  inflating: pos2/4_8.txt            \n",
            "  inflating: pos2/40_8.txt           \n",
            "  inflating: pos2/400_10.txt         \n",
            "  inflating: pos2/401_10.txt         \n",
            "  inflating: pos2/402_10.txt         \n",
            "  inflating: pos2/403_8.txt          \n",
            "  inflating: pos2/404_9.txt          \n",
            "  inflating: pos2/405_10.txt         \n",
            "  inflating: pos2/406_8.txt          \n",
            "  inflating: pos2/407_10.txt         \n",
            "  inflating: pos2/408_10.txt         \n",
            "  inflating: pos2/409_10.txt         \n",
            "  inflating: pos2/41_9.txt           \n",
            "  inflating: pos2/410_8.txt          \n",
            "  inflating: pos2/411_10.txt         \n",
            "  inflating: pos2/412_8.txt          \n",
            "  inflating: pos2/413_10.txt         \n",
            "  inflating: pos2/414_10.txt         \n",
            "  inflating: pos2/415_7.txt          \n",
            "  inflating: pos2/416_8.txt          \n",
            "  inflating: pos2/417_7.txt          \n",
            "  inflating: pos2/418_9.txt          \n",
            "  inflating: pos2/419_7.txt          \n",
            "  inflating: pos2/42_10.txt          \n",
            "  inflating: pos2/420_7.txt          \n",
            "  inflating: pos2/421_9.txt          \n",
            "  inflating: pos2/422_7.txt          \n",
            "  inflating: pos2/423_10.txt         \n",
            "  inflating: pos2/424_8.txt          \n",
            "  inflating: pos2/425_10.txt         \n",
            "  inflating: pos2/426_7.txt          \n",
            "  inflating: pos2/427_10.txt         \n",
            "  inflating: pos2/428_7.txt          \n",
            "  inflating: pos2/429_10.txt         \n",
            "  inflating: pos2/43_10.txt          \n",
            "  inflating: pos2/430_7.txt          \n",
            "  inflating: pos2/431_8.txt          \n",
            "  inflating: pos2/432_8.txt          \n",
            "  inflating: pos2/433_10.txt         \n",
            "  inflating: pos2/434_8.txt          \n",
            "  inflating: pos2/435_8.txt          \n",
            "  inflating: pos2/436_10.txt         \n",
            "  inflating: pos2/437_9.txt          \n",
            "  inflating: pos2/438_9.txt          \n",
            "  inflating: pos2/439_9.txt          \n",
            "  inflating: pos2/44_8.txt           \n",
            "  inflating: pos2/440_10.txt         \n",
            "  inflating: pos2/441_9.txt          \n",
            "  inflating: pos2/442_9.txt          \n",
            "  inflating: pos2/443_10.txt         \n",
            "  inflating: pos2/444_10.txt         \n",
            "  inflating: pos2/445_10.txt         \n",
            "  inflating: pos2/446_10.txt         \n",
            "  inflating: pos2/447_10.txt         \n",
            "  inflating: pos2/448_10.txt         \n",
            "  inflating: pos2/449_10.txt         \n",
            "  inflating: pos2/45_10.txt          \n",
            "  inflating: pos2/450_10.txt         \n",
            "  inflating: pos2/451_10.txt         \n",
            "  inflating: pos2/452_10.txt         \n",
            "  inflating: pos2/453_10.txt         \n",
            "  inflating: pos2/454_8.txt          \n",
            "  inflating: pos2/455_10.txt         \n",
            "  inflating: pos2/456_10.txt         \n",
            "  inflating: pos2/457_10.txt         \n",
            "  inflating: pos2/458_10.txt         \n",
            "  inflating: pos2/459_10.txt         \n",
            "  inflating: pos2/46_9.txt           \n",
            "  inflating: pos2/460_9.txt          \n",
            "  inflating: pos2/461_8.txt          \n",
            "  inflating: pos2/462_8.txt          \n",
            "  inflating: pos2/463_7.txt          \n",
            "  inflating: pos2/464_10.txt         \n",
            "  inflating: pos2/465_10.txt         \n",
            "  inflating: pos2/466_8.txt          \n",
            "  inflating: pos2/467_7.txt          \n",
            "  inflating: pos2/468_7.txt          \n",
            "  inflating: pos2/469_7.txt          \n",
            "  inflating: pos2/47_8.txt           \n",
            "  inflating: pos2/470_10.txt         \n",
            "  inflating: pos2/471_7.txt          \n",
            "  inflating: pos2/472_10.txt         \n",
            "  inflating: pos2/473_9.txt          \n",
            "  inflating: pos2/474_7.txt          \n",
            "  inflating: pos2/475_10.txt         \n",
            "  inflating: pos2/476_7.txt          \n",
            "  inflating: pos2/477_10.txt         \n",
            "  inflating: pos2/478_7.txt          \n",
            "  inflating: pos2/479_10.txt         \n",
            "  inflating: pos2/48_7.txt           \n",
            "  inflating: pos2/480_10.txt         \n",
            "  inflating: pos2/481_10.txt         \n",
            "  inflating: pos2/482_8.txt          \n",
            "  inflating: pos2/483_8.txt          \n",
            "  inflating: pos2/484_8.txt          \n",
            "  inflating: pos2/485_8.txt          \n",
            "  inflating: pos2/486_9.txt          \n",
            "  inflating: pos2/487_8.txt          \n",
            "  inflating: pos2/488_9.txt          \n",
            "  inflating: pos2/489_7.txt          \n",
            "  inflating: pos2/49_10.txt          \n",
            "  inflating: pos2/490_9.txt          \n",
            "  inflating: pos2/491_7.txt          \n",
            "  inflating: pos2/492_7.txt          \n",
            "  inflating: pos2/493_10.txt         \n",
            "  inflating: pos2/494_9.txt          \n",
            "  inflating: pos2/495_7.txt          \n",
            "  inflating: pos2/496_10.txt         \n",
            "  inflating: pos2/497_10.txt         \n",
            "  inflating: pos2/498_10.txt         \n",
            "  inflating: pos2/499_8.txt          \n",
            "  inflating: pos2/5_10.txt           \n",
            "  inflating: pos2/50_10.txt          \n",
            "  inflating: pos2/500_9.txt          \n",
            "  inflating: pos2/51_10.txt          \n",
            "  inflating: pos2/52_10.txt          \n",
            "  inflating: pos2/53_10.txt          \n",
            "  inflating: pos2/54_10.txt          \n",
            "  inflating: pos2/55_9.txt           \n",
            "  inflating: pos2/56_10.txt          \n",
            "  inflating: pos2/57_10.txt          \n",
            "  inflating: pos2/58_9.txt           \n",
            "  inflating: pos2/59_7.txt           \n",
            "  inflating: pos2/6_10.txt           \n",
            "  inflating: pos2/60_8.txt           \n",
            "  inflating: pos2/61_10.txt          \n",
            "  inflating: pos2/62_10.txt          \n",
            "  inflating: pos2/63_10.txt          \n",
            "  inflating: pos2/64_7.txt           \n",
            "  inflating: pos2/65_10.txt          \n",
            "  inflating: pos2/66_8.txt           \n",
            "  inflating: pos2/67_10.txt          \n",
            "  inflating: pos2/68_10.txt          \n",
            "  inflating: pos2/69_10.txt          \n",
            "  inflating: pos2/7_7.txt            \n",
            "  inflating: pos2/70_9.txt           \n",
            "  inflating: pos2/71_10.txt          \n",
            "  inflating: pos2/72_7.txt           \n",
            "  inflating: pos2/73_7.txt           \n",
            "  inflating: pos2/74_8.txt           \n",
            "  inflating: pos2/75_8.txt           \n",
            "  inflating: pos2/76_7.txt           \n",
            "  inflating: pos2/77_7.txt           \n",
            "  inflating: pos2/78_10.txt          \n",
            "  inflating: pos2/79_10.txt          \n",
            "  inflating: pos2/8_7.txt            \n",
            "  inflating: pos2/80_9.txt           \n",
            "  inflating: pos2/81_10.txt          \n",
            "  inflating: pos2/82_8.txt           \n",
            "  inflating: pos2/83_10.txt          \n",
            "  inflating: pos2/84_10.txt          \n",
            "  inflating: pos2/85_10.txt          \n",
            "  inflating: pos2/86_10.txt          \n",
            "  inflating: pos2/87_10.txt          \n",
            "  inflating: pos2/88_9.txt           \n",
            "  inflating: pos2/89_7.txt           \n",
            "  inflating: pos2/9_7.txt            \n",
            "  inflating: pos2/90_7.txt           \n",
            "  inflating: pos2/91_8.txt           \n",
            "  inflating: pos2/92_9.txt           \n",
            "  inflating: pos2/93_10.txt          \n",
            "  inflating: pos2/94_10.txt          \n",
            "  inflating: pos2/95_10.txt          \n",
            "  inflating: pos2/96_10.txt          \n",
            "  inflating: pos2/97_9.txt           \n",
            "  inflating: pos2/98_10.txt          \n",
            "  inflating: pos2/99_8.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtFT8Jv4mfrM",
        "outputId": "663915eb-3cc3-48c2-ae79-05f7a450ef87"
      },
      "source": [
        "!unzip /content/neg2.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/neg2.zip\n",
            "  inflating: neg2/0_3.txt            \n",
            "  inflating: neg2/1_1.txt            \n",
            "  inflating: neg2/10_2.txt           \n",
            "  inflating: neg2/100_3.txt          \n",
            "  inflating: neg2/101_1.txt          \n",
            "  inflating: neg2/102_1.txt          \n",
            "  inflating: neg2/103_1.txt          \n",
            "  inflating: neg2/104_3.txt          \n",
            "  inflating: neg2/105_2.txt          \n",
            "  inflating: neg2/106_2.txt          \n",
            "  inflating: neg2/107_2.txt          \n",
            "  inflating: neg2/108_1.txt          \n",
            "  inflating: neg2/109_2.txt          \n",
            "  inflating: neg2/11_3.txt           \n",
            "  inflating: neg2/110_1.txt          \n",
            "  inflating: neg2/111_4.txt          \n",
            "  inflating: neg2/112_1.txt          \n",
            "  inflating: neg2/113_4.txt          \n",
            "  inflating: neg2/114_4.txt          \n",
            "  inflating: neg2/115_2.txt          \n",
            "  inflating: neg2/116_1.txt          \n",
            "  inflating: neg2/117_3.txt          \n",
            "  inflating: neg2/118_2.txt          \n",
            "  inflating: neg2/119_4.txt          \n",
            "  inflating: neg2/12_1.txt           \n",
            "  inflating: neg2/120_1.txt          \n",
            "  inflating: neg2/121_4.txt          \n",
            "  inflating: neg2/122_1.txt          \n",
            "  inflating: neg2/123_1.txt          \n",
            "  inflating: neg2/124_2.txt          \n",
            "  inflating: neg2/125_1.txt          \n",
            "  inflating: neg2/126_1.txt          \n",
            "  inflating: neg2/127_4.txt          \n",
            "  inflating: neg2/128_4.txt          \n",
            "  inflating: neg2/129_3.txt          \n",
            "  inflating: neg2/13_2.txt           \n",
            "  inflating: neg2/130_1.txt          \n",
            "  inflating: neg2/131_4.txt          \n",
            "  inflating: neg2/132_3.txt          \n",
            "  inflating: neg2/133_2.txt          \n",
            "  inflating: neg2/134_2.txt          \n",
            "  inflating: neg2/135_4.txt          \n",
            "  inflating: neg2/136_4.txt          \n",
            "  inflating: neg2/137_4.txt          \n",
            "  inflating: neg2/138_4.txt          \n",
            "  inflating: neg2/139_4.txt          \n",
            "  inflating: neg2/14_2.txt           \n",
            "  inflating: neg2/140_2.txt          \n",
            "  inflating: neg2/141_3.txt          \n",
            "  inflating: neg2/142_3.txt          \n",
            "  inflating: neg2/143_2.txt          \n",
            "  inflating: neg2/144_2.txt          \n",
            "  inflating: neg2/145_2.txt          \n",
            "  inflating: neg2/146_2.txt          \n",
            "  inflating: neg2/147_4.txt          \n",
            "  inflating: neg2/148_2.txt          \n",
            "  inflating: neg2/149_1.txt          \n",
            "  inflating: neg2/15_1.txt           \n",
            "  inflating: neg2/150_1.txt          \n",
            "  inflating: neg2/151_1.txt          \n",
            "  inflating: neg2/152_4.txt          \n",
            "  inflating: neg2/153_1.txt          \n",
            "  inflating: neg2/154_3.txt          \n",
            "  inflating: neg2/155_3.txt          \n",
            "  inflating: neg2/156_1.txt          \n",
            "  inflating: neg2/157_1.txt          \n",
            "  inflating: neg2/158_3.txt          \n",
            "  inflating: neg2/159_4.txt          \n",
            "  inflating: neg2/16_3.txt           \n",
            "  inflating: neg2/160_2.txt          \n",
            "  inflating: neg2/161_2.txt          \n",
            "  inflating: neg2/162_4.txt          \n",
            "  inflating: neg2/163_4.txt          \n",
            "  inflating: neg2/164_3.txt          \n",
            "  inflating: neg2/165_4.txt          \n",
            "  inflating: neg2/166_1.txt          \n",
            "  inflating: neg2/167_1.txt          \n",
            "  inflating: neg2/168_1.txt          \n",
            "  inflating: neg2/169_1.txt          \n",
            "  inflating: neg2/17_3.txt           \n",
            "  inflating: neg2/170_1.txt          \n",
            "  inflating: neg2/171_1.txt          \n",
            "  inflating: neg2/172_1.txt          \n",
            "  inflating: neg2/173_3.txt          \n",
            "  inflating: neg2/174_3.txt          \n",
            "  inflating: neg2/175_1.txt          \n",
            "  inflating: neg2/176_4.txt          \n",
            "  inflating: neg2/177_1.txt          \n",
            "  inflating: neg2/178_3.txt          \n",
            "  inflating: neg2/179_3.txt          \n",
            "  inflating: neg2/18_3.txt           \n",
            "  inflating: neg2/180_4.txt          \n",
            "  inflating: neg2/181_2.txt          \n",
            "  inflating: neg2/182_1.txt          \n",
            "  inflating: neg2/183_3.txt          \n",
            "  inflating: neg2/184_2.txt          \n",
            "  inflating: neg2/185_4.txt          \n",
            "  inflating: neg2/186_1.txt          \n",
            "  inflating: neg2/187_2.txt          \n",
            "  inflating: neg2/188_1.txt          \n",
            "  inflating: neg2/189_1.txt          \n",
            "  inflating: neg2/19_4.txt           \n",
            "  inflating: neg2/190_1.txt          \n",
            "  inflating: neg2/191_3.txt          \n",
            "  inflating: neg2/192_2.txt          \n",
            "  inflating: neg2/193_1.txt          \n",
            "  inflating: neg2/194_1.txt          \n",
            "  inflating: neg2/195_4.txt          \n",
            "  inflating: neg2/196_3.txt          \n",
            "  inflating: neg2/197_3.txt          \n",
            "  inflating: neg2/198_2.txt          \n",
            "  inflating: neg2/199_1.txt          \n",
            "  inflating: neg2/2_1.txt            \n",
            "  inflating: neg2/20_1.txt           \n",
            "  inflating: neg2/200_1.txt          \n",
            "  inflating: neg2/201_4.txt          \n",
            "  inflating: neg2/202_2.txt          \n",
            "  inflating: neg2/203_1.txt          \n",
            "  inflating: neg2/204_4.txt          \n",
            "  inflating: neg2/205_4.txt          \n",
            "  inflating: neg2/206_2.txt          \n",
            "  inflating: neg2/207_1.txt          \n",
            "  inflating: neg2/208_1.txt          \n",
            "  inflating: neg2/209_1.txt          \n",
            "  inflating: neg2/21_4.txt           \n",
            "  inflating: neg2/210_4.txt          \n",
            "  inflating: neg2/211_4.txt          \n",
            "  inflating: neg2/212_4.txt          \n",
            "  inflating: neg2/213_4.txt          \n",
            "  inflating: neg2/214_4.txt          \n",
            "  inflating: neg2/215_4.txt          \n",
            "  inflating: neg2/216_4.txt          \n",
            "  inflating: neg2/217_3.txt          \n",
            "  inflating: neg2/218_4.txt          \n",
            "  inflating: neg2/219_2.txt          \n",
            "  inflating: neg2/22_1.txt           \n",
            "  inflating: neg2/220_4.txt          \n",
            "  inflating: neg2/221_4.txt          \n",
            "  inflating: neg2/222_1.txt          \n",
            "  inflating: neg2/223_1.txt          \n",
            "  inflating: neg2/224_4.txt          \n",
            "  inflating: neg2/225_2.txt          \n",
            "  inflating: neg2/226_4.txt          \n",
            "  inflating: neg2/227_1.txt          \n",
            "  inflating: neg2/228_1.txt          \n",
            "  inflating: neg2/229_1.txt          \n",
            "  inflating: neg2/23_3.txt           \n",
            "  inflating: neg2/230_2.txt          \n",
            "  inflating: neg2/231_1.txt          \n",
            "  inflating: neg2/232_1.txt          \n",
            "  inflating: neg2/233_1.txt          \n",
            "  inflating: neg2/234_1.txt          \n",
            "  inflating: neg2/235_1.txt          \n",
            "  inflating: neg2/236_1.txt          \n",
            "  inflating: neg2/237_1.txt          \n",
            "  inflating: neg2/238_4.txt          \n",
            "  inflating: neg2/239_2.txt          \n",
            "  inflating: neg2/24_1.txt           \n",
            "  inflating: neg2/240_1.txt          \n",
            "  inflating: neg2/241_1.txt          \n",
            "  inflating: neg2/242_1.txt          \n",
            "  inflating: neg2/243_3.txt          \n",
            "  inflating: neg2/244_4.txt          \n",
            "  inflating: neg2/245_1.txt          \n",
            "  inflating: neg2/246_3.txt          \n",
            "  inflating: neg2/247_3.txt          \n",
            "  inflating: neg2/248_4.txt          \n",
            "  inflating: neg2/249_3.txt          \n",
            "  inflating: neg2/25_1.txt           \n",
            "  inflating: neg2/250_3.txt          \n",
            "  inflating: neg2/251_3.txt          \n",
            "  inflating: neg2/252_4.txt          \n",
            "  inflating: neg2/253_3.txt          \n",
            "  inflating: neg2/254_1.txt          \n",
            "  inflating: neg2/255_3.txt          \n",
            "  inflating: neg2/256_1.txt          \n",
            "  inflating: neg2/257_2.txt          \n",
            "  inflating: neg2/258_4.txt          \n",
            "  inflating: neg2/259_3.txt          \n",
            "  inflating: neg2/26_3.txt           \n",
            "  inflating: neg2/260_4.txt          \n",
            "  inflating: neg2/261_4.txt          \n",
            "  inflating: neg2/262_4.txt          \n",
            "  inflating: neg2/263_4.txt          \n",
            "  inflating: neg2/264_2.txt          \n",
            "  inflating: neg2/265_1.txt          \n",
            "  inflating: neg2/266_3.txt          \n",
            "  inflating: neg2/267_3.txt          \n",
            "  inflating: neg2/268_1.txt          \n",
            "  inflating: neg2/269_1.txt          \n",
            "  inflating: neg2/27_1.txt           \n",
            "  inflating: neg2/270_1.txt          \n",
            "  inflating: neg2/271_1.txt          \n",
            "  inflating: neg2/272_2.txt          \n",
            "  inflating: neg2/273_1.txt          \n",
            "  inflating: neg2/274_2.txt          \n",
            "  inflating: neg2/275_3.txt          \n",
            "  inflating: neg2/276_2.txt          \n",
            "  inflating: neg2/277_4.txt          \n",
            "  inflating: neg2/278_1.txt          \n",
            "  inflating: neg2/279_1.txt          \n",
            "  inflating: neg2/28_2.txt           \n",
            "  inflating: neg2/280_2.txt          \n",
            "  inflating: neg2/281_1.txt          \n",
            "  inflating: neg2/282_1.txt          \n",
            "  inflating: neg2/283_1.txt          \n",
            "  inflating: neg2/284_2.txt          \n",
            "  inflating: neg2/285_3.txt          \n",
            "  inflating: neg2/286_1.txt          \n",
            "  inflating: neg2/287_4.txt          \n",
            "  inflating: neg2/288_1.txt          \n",
            "  inflating: neg2/289_3.txt          \n",
            "  inflating: neg2/29_4.txt           \n",
            "  inflating: neg2/290_2.txt          \n",
            "  inflating: neg2/291_3.txt          \n",
            "  inflating: neg2/292_1.txt          \n",
            "  inflating: neg2/293_2.txt          \n",
            "  inflating: neg2/294_4.txt          \n",
            "  inflating: neg2/295_3.txt          \n",
            "  inflating: neg2/296_2.txt          \n",
            "  inflating: neg2/297_4.txt          \n",
            "  inflating: neg2/298_4.txt          \n",
            "  inflating: neg2/299_1.txt          \n",
            "  inflating: neg2/3_4.txt            \n",
            "  inflating: neg2/30_1.txt           \n",
            "  inflating: neg2/300_4.txt          \n",
            "  inflating: neg2/301_1.txt          \n",
            "  inflating: neg2/302_4.txt          \n",
            "  inflating: neg2/303_2.txt          \n",
            "  inflating: neg2/304_4.txt          \n",
            "  inflating: neg2/305_1.txt          \n",
            "  inflating: neg2/306_1.txt          \n",
            "  inflating: neg2/307_1.txt          \n",
            "  inflating: neg2/308_1.txt          \n",
            "  inflating: neg2/309_1.txt          \n",
            "  inflating: neg2/31_1.txt           \n",
            "  inflating: neg2/310_1.txt          \n",
            "  inflating: neg2/311_1.txt          \n",
            "  inflating: neg2/312_2.txt          \n",
            "  inflating: neg2/313_2.txt          \n",
            "  inflating: neg2/314_1.txt          \n",
            "  inflating: neg2/315_1.txt          \n",
            "  inflating: neg2/316_1.txt          \n",
            "  inflating: neg2/317_1.txt          \n",
            "  inflating: neg2/318_1.txt          \n",
            "  inflating: neg2/319_1.txt          \n",
            "  inflating: neg2/32_3.txt           \n",
            "  inflating: neg2/320_1.txt          \n",
            "  inflating: neg2/321_1.txt          \n",
            "  inflating: neg2/322_1.txt          \n",
            "  inflating: neg2/323_1.txt          \n",
            "  inflating: neg2/324_1.txt          \n",
            "  inflating: neg2/325_2.txt          \n",
            "  inflating: neg2/326_1.txt          \n",
            "  inflating: neg2/327_3.txt          \n",
            "  inflating: neg2/328_1.txt          \n",
            "  inflating: neg2/329_1.txt          \n",
            "  inflating: neg2/33_3.txt           \n",
            "  inflating: neg2/330_1.txt          \n",
            "  inflating: neg2/331_1.txt          \n",
            "  inflating: neg2/332_1.txt          \n",
            "  inflating: neg2/333_3.txt          \n",
            "  inflating: neg2/334_4.txt          \n",
            "  inflating: neg2/335_4.txt          \n",
            "  inflating: neg2/336_1.txt          \n",
            "  inflating: neg2/337_4.txt          \n",
            "  inflating: neg2/338_4.txt          \n",
            "  inflating: neg2/339_4.txt          \n",
            "  inflating: neg2/34_1.txt           \n",
            "  inflating: neg2/340_3.txt          \n",
            "  inflating: neg2/341_4.txt          \n",
            "  inflating: neg2/342_1.txt          \n",
            "  inflating: neg2/343_2.txt          \n",
            "  inflating: neg2/344_2.txt          \n",
            "  inflating: neg2/345_4.txt          \n",
            "  inflating: neg2/346_1.txt          \n",
            "  inflating: neg2/347_1.txt          \n",
            "  inflating: neg2/348_2.txt          \n",
            "  inflating: neg2/349_4.txt          \n",
            "  inflating: neg2/35_3.txt           \n",
            "  inflating: neg2/350_2.txt          \n",
            "  inflating: neg2/351_4.txt          \n",
            "  inflating: neg2/352_4.txt          \n",
            "  inflating: neg2/353_4.txt          \n",
            "  inflating: neg2/354_3.txt          \n",
            "  inflating: neg2/355_4.txt          \n",
            "  inflating: neg2/356_4.txt          \n",
            "  inflating: neg2/357_2.txt          \n",
            "  inflating: neg2/358_4.txt          \n",
            "  inflating: neg2/359_3.txt          \n",
            "  inflating: neg2/36_4.txt           \n",
            "  inflating: neg2/360_4.txt          \n",
            "  inflating: neg2/361_4.txt          \n",
            "  inflating: neg2/362_3.txt          \n",
            "  inflating: neg2/363_1.txt          \n",
            "  inflating: neg2/364_1.txt          \n",
            "  inflating: neg2/365_1.txt          \n",
            "  inflating: neg2/366_1.txt          \n",
            "  inflating: neg2/367_2.txt          \n",
            "  inflating: neg2/368_4.txt          \n",
            "  inflating: neg2/369_1.txt          \n",
            "  inflating: neg2/37_3.txt           \n",
            "  inflating: neg2/370_1.txt          \n",
            "  inflating: neg2/371_1.txt          \n",
            "  inflating: neg2/372_1.txt          \n",
            "  inflating: neg2/373_4.txt          \n",
            "  inflating: neg2/374_2.txt          \n",
            "  inflating: neg2/375_2.txt          \n",
            "  inflating: neg2/376_4.txt          \n",
            "  inflating: neg2/377_1.txt          \n",
            "  inflating: neg2/378_4.txt          \n",
            "  inflating: neg2/379_2.txt          \n",
            "  inflating: neg2/38_2.txt           \n",
            "  inflating: neg2/380_4.txt          \n",
            "  inflating: neg2/381_1.txt          \n",
            "  inflating: neg2/382_1.txt          \n",
            "  inflating: neg2/383_4.txt          \n",
            "  inflating: neg2/384_4.txt          \n",
            "  inflating: neg2/385_3.txt          \n",
            "  inflating: neg2/386_4.txt          \n",
            "  inflating: neg2/387_2.txt          \n",
            "  inflating: neg2/388_1.txt          \n",
            "  inflating: neg2/389_3.txt          \n",
            "  inflating: neg2/39_2.txt           \n",
            "  inflating: neg2/390_4.txt          \n",
            "  inflating: neg2/391_4.txt          \n",
            "  inflating: neg2/392_3.txt          \n",
            "  inflating: neg2/393_2.txt          \n",
            "  inflating: neg2/394_3.txt          \n",
            "  inflating: neg2/395_1.txt          \n",
            "  inflating: neg2/396_3.txt          \n",
            "  inflating: neg2/397_3.txt          \n",
            "  inflating: neg2/398_3.txt          \n",
            "  inflating: neg2/399_2.txt          \n",
            "  inflating: neg2/4_4.txt            \n",
            "  inflating: neg2/40_3.txt           \n",
            "  inflating: neg2/400_2.txt          \n",
            "  inflating: neg2/401_3.txt          \n",
            "  inflating: neg2/402_2.txt          \n",
            "  inflating: neg2/403_3.txt          \n",
            "  inflating: neg2/404_2.txt          \n",
            "  inflating: neg2/405_3.txt          \n",
            "  inflating: neg2/406_2.txt          \n",
            "  inflating: neg2/407_4.txt          \n",
            "  inflating: neg2/408_2.txt          \n",
            "  inflating: neg2/409_2.txt          \n",
            "  inflating: neg2/41_1.txt           \n",
            "  inflating: neg2/410_4.txt          \n",
            "  inflating: neg2/411_2.txt          \n",
            "  inflating: neg2/412_4.txt          \n",
            "  inflating: neg2/413_3.txt          \n",
            "  inflating: neg2/414_2.txt          \n",
            "  inflating: neg2/415_3.txt          \n",
            "  inflating: neg2/416_4.txt          \n",
            "  inflating: neg2/417_1.txt          \n",
            "  inflating: neg2/418_4.txt          \n",
            "  inflating: neg2/419_2.txt          \n",
            "  inflating: neg2/42_3.txt           \n",
            "  inflating: neg2/420_4.txt          \n",
            "  inflating: neg2/421_4.txt          \n",
            "  inflating: neg2/422_1.txt          \n",
            "  inflating: neg2/423_4.txt          \n",
            "  inflating: neg2/424_4.txt          \n",
            "  inflating: neg2/425_2.txt          \n",
            "  inflating: neg2/426_3.txt          \n",
            "  inflating: neg2/427_4.txt          \n",
            "  inflating: neg2/428_1.txt          \n",
            "  inflating: neg2/429_3.txt          \n",
            "  inflating: neg2/43_4.txt           \n",
            "  inflating: neg2/430_1.txt          \n",
            "  inflating: neg2/431_4.txt          \n",
            "  inflating: neg2/432_4.txt          \n",
            "  inflating: neg2/433_4.txt          \n",
            "  inflating: neg2/434_4.txt          \n",
            "  inflating: neg2/435_2.txt          \n",
            "  inflating: neg2/436_1.txt          \n",
            "  inflating: neg2/437_4.txt          \n",
            "  inflating: neg2/438_4.txt          \n",
            "  inflating: neg2/439_1.txt          \n",
            "  inflating: neg2/44_2.txt           \n",
            "  inflating: neg2/440_3.txt          \n",
            "  inflating: neg2/441_2.txt          \n",
            "  inflating: neg2/442_1.txt          \n",
            "  inflating: neg2/443_3.txt          \n",
            "  inflating: neg2/444_4.txt          \n",
            "  inflating: neg2/445_4.txt          \n",
            "  inflating: neg2/446_3.txt          \n",
            "  inflating: neg2/447_1.txt          \n",
            "  inflating: neg2/448_1.txt          \n",
            "  inflating: neg2/449_4.txt          \n",
            "  inflating: neg2/45_2.txt           \n",
            "  inflating: neg2/450_1.txt          \n",
            "  inflating: neg2/451_2.txt          \n",
            "  inflating: neg2/452_3.txt          \n",
            "  inflating: neg2/453_3.txt          \n",
            "  inflating: neg2/454_4.txt          \n",
            "  inflating: neg2/455_4.txt          \n",
            "  inflating: neg2/456_1.txt          \n",
            "  inflating: neg2/457_3.txt          \n",
            "  inflating: neg2/458_1.txt          \n",
            "  inflating: neg2/459_1.txt          \n",
            "  inflating: neg2/46_4.txt           \n",
            "  inflating: neg2/460_2.txt          \n",
            "  inflating: neg2/461_1.txt          \n",
            "  inflating: neg2/462_2.txt          \n",
            "  inflating: neg2/463_4.txt          \n",
            "  inflating: neg2/464_2.txt          \n",
            "  inflating: neg2/465_1.txt          \n",
            "  inflating: neg2/466_1.txt          \n",
            "  inflating: neg2/467_1.txt          \n",
            "  inflating: neg2/468_4.txt          \n",
            "  inflating: neg2/469_2.txt          \n",
            "  inflating: neg2/47_2.txt           \n",
            "  inflating: neg2/470_3.txt          \n",
            "  inflating: neg2/471_4.txt          \n",
            "  inflating: neg2/472_1.txt          \n",
            "  inflating: neg2/473_1.txt          \n",
            "  inflating: neg2/474_4.txt          \n",
            "  inflating: neg2/475_1.txt          \n",
            "  inflating: neg2/476_1.txt          \n",
            "  inflating: neg2/477_4.txt          \n",
            "  inflating: neg2/478_2.txt          \n",
            "  inflating: neg2/479_3.txt          \n",
            "  inflating: neg2/48_4.txt           \n",
            "  inflating: neg2/480_3.txt          \n",
            "  inflating: neg2/481_3.txt          \n",
            "  inflating: neg2/482_1.txt          \n",
            "  inflating: neg2/483_2.txt          \n",
            "  inflating: neg2/484_1.txt          \n",
            "  inflating: neg2/485_3.txt          \n",
            "  inflating: neg2/486_1.txt          \n",
            "  inflating: neg2/487_4.txt          \n",
            "  inflating: neg2/488_4.txt          \n",
            "  inflating: neg2/489_4.txt          \n",
            "  inflating: neg2/49_4.txt           \n",
            "  inflating: neg2/490_3.txt          \n",
            "  inflating: neg2/491_1.txt          \n",
            "  inflating: neg2/492_4.txt          \n",
            "  inflating: neg2/493_4.txt          \n",
            "  inflating: neg2/494_3.txt          \n",
            "  inflating: neg2/495_1.txt          \n",
            "  inflating: neg2/496_1.txt          \n",
            "  inflating: neg2/497_2.txt          \n",
            "  inflating: neg2/498_3.txt          \n",
            "  inflating: neg2/499_1.txt          \n",
            "  inflating: neg2/5_3.txt            \n",
            "  inflating: neg2/50_4.txt           \n",
            "  inflating: neg2/500_2.txt          \n",
            "  inflating: neg2/51_1.txt           \n",
            "  inflating: neg2/52_1.txt           \n",
            "  inflating: neg2/53_3.txt           \n",
            "  inflating: neg2/54_1.txt           \n",
            "  inflating: neg2/55_1.txt           \n",
            "  inflating: neg2/56_3.txt           \n",
            "  inflating: neg2/57_4.txt           \n",
            "  inflating: neg2/58_3.txt           \n",
            "  inflating: neg2/59_3.txt           \n",
            "  inflating: neg2/6_1.txt            \n",
            "  inflating: neg2/60_4.txt           \n",
            "  inflating: neg2/61_3.txt           \n",
            "  inflating: neg2/62_2.txt           \n",
            "  inflating: neg2/63_1.txt           \n",
            "  inflating: neg2/64_1.txt           \n",
            "  inflating: neg2/65_4.txt           \n",
            "  inflating: neg2/66_4.txt           \n",
            "  inflating: neg2/67_2.txt           \n",
            "  inflating: neg2/68_2.txt           \n",
            "  inflating: neg2/69_4.txt           \n",
            "  inflating: neg2/7_3.txt            \n",
            "  inflating: neg2/70_2.txt           \n",
            "  inflating: neg2/71_1.txt           \n",
            "  inflating: neg2/72_4.txt           \n",
            "  inflating: neg2/73_1.txt           \n",
            "  inflating: neg2/74_3.txt           \n",
            "  inflating: neg2/75_1.txt           \n",
            "  inflating: neg2/76_3.txt           \n",
            "  inflating: neg2/77_4.txt           \n",
            "  inflating: neg2/78_4.txt           \n",
            "  inflating: neg2/79_4.txt           \n",
            "  inflating: neg2/8_4.txt            \n",
            "  inflating: neg2/80_3.txt           \n",
            "  inflating: neg2/81_1.txt           \n",
            "  inflating: neg2/82_1.txt           \n",
            "  inflating: neg2/83_3.txt           \n",
            "  inflating: neg2/84_3.txt           \n",
            "  inflating: neg2/85_2.txt           \n",
            "  inflating: neg2/86_4.txt           \n",
            "  inflating: neg2/87_4.txt           \n",
            "  inflating: neg2/88_2.txt           \n",
            "  inflating: neg2/89_2.txt           \n",
            "  inflating: neg2/9_1.txt            \n",
            "  inflating: neg2/90_4.txt           \n",
            "  inflating: neg2/91_1.txt           \n",
            "  inflating: neg2/92_3.txt           \n",
            "  inflating: neg2/93_1.txt           \n",
            "  inflating: neg2/94_1.txt           \n",
            "  inflating: neg2/95_3.txt           \n",
            "  inflating: neg2/96_1.txt           \n",
            "  inflating: neg2/97_1.txt           \n",
            "  inflating: neg2/98_1.txt           \n",
            "  inflating: neg2/99_1.txt           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyVNniCYmkes"
      },
      "source": [
        "def pre_process_data(filepath):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "    Load pos and neg examples from separate dirs then shuffle them together\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  positive_path = os.path.join(filepath, 'pos2')\r\n",
        "  negative_path = os.path.join(filepath, 'neg2')\r\n",
        "\r\n",
        "  pos_label = 1\r\n",
        "  neg_label = 0\r\n",
        "\r\n",
        "  dataset = []\r\n",
        "\r\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\r\n",
        "    with open(filename, 'r') as f:\r\n",
        "      dataset.append((pos_label, f.read()))\r\n",
        "\r\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\r\n",
        "    with open(filename, 'r') as f:\r\n",
        "      dataset.append((neg_label, f.read()))\r\n",
        "\r\n",
        "  shuffle(dataset)\r\n",
        "\r\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Vxc_ciot10"
      },
      "source": [
        "dataset = pre_process_data(\"/content/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPshaLW7o344",
        "outputId": "d5bb825f-cae3-4154-f677-8ff2b2af67d3"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, \"I watched this movie for a project on love. please tell Nicolas Cage to learn what it would feel like to be his character, and then re-read the lines he's saying. My life cannot go on... i accidentally cut off my own hand...my brother was close by. Obviously his fault. And since when have happy endings included the nice guy who takes care of Mom sad and alone. No closure, bad script, and doesn't have enough extension of minor characters. Save yourself, unless your up for a good laugh. Costumes were done appropriately, and extras did a fabulous job. I'm sure it would have been a fun movie to make, but keep it more genre specific, I can't recommend this movie to anyone I know, because it is not an intellectual movie. It is not a chick flick. It is not a strict romantic. And I can't show kids because of the sex and questions to follow. All in all, just not a good flick.\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vjVM_ZPo8zY"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\r\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdfswDmEpQD3"
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WiX1XVfpa9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0f341e-c676-486f-dc7a-3d396248744c"
      },
      "source": [
        "word_vecs = api.load(\"word2vec-google-news-300\", return_path=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 100.0% 1662.1/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7jwDRKopwjW",
        "outputId": "2db0285f-db75-4e45-99ac-3c6d54d7ef88"
      },
      "source": [
        "print(word_vecs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTLHBo8Bq-Or"
      },
      "source": [
        "word_vectors = KeyedVectors.load_word2vec_format(word_vecs, binary=True, limit=200000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNhARXIYrT2U"
      },
      "source": [
        "def tokenize_and_vectorize(dataset):\r\n",
        "  tokenizer = TreebankWordTokenizer()\r\n",
        "  vectorized_data = []\r\n",
        "  for sample in dataset:\r\n",
        "    tokens = tokenizer.tokenize(sample[1])\r\n",
        "    sample_vecs = []\r\n",
        "    for token in tokens:\r\n",
        "      try:\r\n",
        "        sample_vecs.append(word_vectors[token])\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "      \r\n",
        "    vectorized_data.append(sample_vecs)\r\n",
        "\r\n",
        "  return vectorized_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiwXdKtzv5P"
      },
      "source": [
        "def collect_expected(dataset):\r\n",
        "  \"\"\"Peel off the target values from the dataset\"\"\"\r\n",
        "  expected=[]\r\n",
        "  for sample in dataset:\r\n",
        "    expected.append(sample[0])\r\n",
        "  return expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A54q-qCP1Hej"
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\r\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PenNweh82m0C"
      },
      "source": [
        "split_point = int(len(vectorized_data) * .8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIET63p83vL-"
      },
      "source": [
        "X_train = vectorized_data[:split_point]\r\n",
        "y_train = expected[:split_point]\r\n",
        "X_test = vectorized_data[split_point:]\r\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUEpeMnA4FRO"
      },
      "source": [
        "maxlen = 400\r\n",
        "batch_size = 8\r\n",
        "embedding_dims = 300\r\n",
        "epochs = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qItnMyg24YQt"
      },
      "source": [
        "def pad_trunc(data, maxlen):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "    For a given dataset pad with zero vectors or truncate to maxlen\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  new_data = []\r\n",
        "\r\n",
        "  # Create a vector of 0s the lenght of our word vectors\r\n",
        "\r\n",
        "  zero_vector = []\r\n",
        "  for _ in range(len(data[0][0])):\r\n",
        "    zero_vector.append(0.0)\r\n",
        "\r\n",
        "  for sample in data:\r\n",
        "    if len(sample) > maxlen:\r\n",
        "      temp = sample[:maxlen]\r\n",
        "    elif len(sample) < maxlen:\r\n",
        "      temp = sample\r\n",
        "      # Append the appropriate number 0 vectors to the list\r\n",
        "      additional_elems = maxlen - len(sample)\r\n",
        "      for _ in range(additional_elems):\r\n",
        "        temp.append(zero_vector)\r\n",
        "\r\n",
        "    else:\r\n",
        "      temp = sample\r\n",
        "\r\n",
        "    new_data.append(temp)\r\n",
        "\r\n",
        "  return new_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsVfijm79kI3",
        "outputId": "4b55f6ad-9464-47c9-8731-dd347f1dc30c"
      },
      "source": [
        "len(X_train[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2t5cH379q5D",
        "outputId": "1f3deb67-ed68-4d91-fd97-43f93dd3c36e"
      },
      "source": [
        "expected[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InNISClU_de0"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "X_train = pad_trunc(X_train, maxlen)\r\n",
        "X_test = pad_trunc(X_test, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Z4uc-iBb79",
        "outputId": "a09474c2-bb47-4768-c060-86ca0f5b7729"
      },
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk301GJFBq3t",
        "outputId": "214498b2-83e4-4cf7-88fb-e620e8e1af4f"
      },
      "source": [
        "len(X_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-1xiIICI5I"
      },
      "source": [
        "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxUSFV2-Btvc"
      },
      "source": [
        "\r\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0oGRBmJCC0c"
      },
      "source": [
        "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))\r\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eClWzXlEoiq",
        "outputId": "d1ac8a6a-241a-4cfc-f85e-8bed05cc81b7"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(801, 400, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-odNzKfXEz03",
        "outputId": "cb6f370d-28f7-4f4c-818a-8d1c85569080"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(201, 400, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkcsB6hE1u3"
      },
      "source": [
        "from keras.models import Sequential \r\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM \r\n",
        "num_neurons = 50\r\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtlt14ZkBpqQ",
        "outputId": "61d5b7bb-ca45-4cbd-d34e-af5973a4a4c7"
      },
      "source": [
        "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\r\n",
        "model.add(Dropout(.2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 400, 50)           70200     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 20001     \n",
            "=================================================================\n",
            "Total params: 90,201\n",
            "Trainable params: 90,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFhTrARTCSC3",
        "outputId": "8cbbde6b-e7d1-4bc2-9b82-c27fa0d45d1d"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "101/101 [==============================] - 16s 161ms/step - loss: 0.6594 - accuracy: 0.6055 - val_loss: 0.7793 - val_accuracy: 0.5025\n",
            "Epoch 2/3\n",
            "101/101 [==============================] - 16s 159ms/step - loss: 0.4652 - accuracy: 0.7890 - val_loss: 0.5195 - val_accuracy: 0.7363\n",
            "Epoch 3/3\n",
            "101/101 [==============================] - 16s 162ms/step - loss: 0.3025 - accuracy: 0.8851 - val_loss: 0.4721 - val_accuracy: 0.7761\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3234c48a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NcKsbhkCpsj"
      },
      "source": [
        "model_structure = model.to_json()\r\n",
        "with open(\"lstm_model1.json\", \"w\") as json_file:\r\n",
        "  json_file.write(model_structure)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKvXAM1xDw3K"
      },
      "source": [
        "model.save_weights(\"lstm_weights1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wgMw-jnD2ES"
      },
      "source": [
        "from keras.models import model_from_json\r\n",
        "with open(\"lstm_model1.json\", \"r\") as json_file:\r\n",
        "  json_string = json_file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fau8SsZCHTHm"
      },
      "source": [
        "model = model_from_json(json_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbC7qTHHXrr"
      },
      "source": [
        "model.load_weights(\"/content/lstm_weights1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0tdESN-Hicr"
      },
      "source": [
        "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIt20EBzIEfs"
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p-vQIqtIPda"
      },
      "source": [
        "test_vec_list = pad_trunc(vec_list, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2WG4RU0InG6"
      },
      "source": [
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYjxH15II0_Y",
        "outputId": "f448594f-7721-4bd9-c5da-ffd2f4c0f845"
      },
      "source": [
        "print(\"Sample's sentiment, 1-pos, 2-neg: {}\".format(model.predict_classes(test_vec)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-48-0ac54ed20f6e>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "Sample's sentiment, 1-pos, 2-neg: [[0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4lYPh9zJInI",
        "outputId": "de911b85-b6ac-4348-b945-55f03a74cd81"
      },
      "source": [
        "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw output of sigmoid function: [[0.17224404]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bygdMcv3JZiN"
      },
      "source": [
        "def test_lan(data, maxlen):\r\n",
        "  total_len = truncated = exact = padded = 0\r\n",
        "  for sample in data:\r\n",
        "    total_len = total_len + len(sample)\r\n",
        "    if len(sample) > maxlen:\r\n",
        "      truncated = truncated + 1\r\n",
        "    elif len(sample) < maxlen:\r\n",
        "      padded = padded + 1\r\n",
        "    else:\r\n",
        "      exact = exact + 1\r\n",
        "\r\n",
        "  print('Padded: {}'.format(padded))\r\n",
        "  print('Equal: {}'.format(exact))\r\n",
        "  print('Truncated: {}'.format(truncated))\r\n",
        "  print('Avg length: {}'.format(total_len/len(data)))\r\n",
        "  print('Total length: {}'.format(total_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RCASwxQYAab",
        "outputId": "4c9cd04d-85ca-4770-a2a4-180943f4f847"
      },
      "source": [
        "test_lan(vectorized_data, 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded: 0\n",
            "Equal: 917\n",
            "Truncated: 85\n",
            "Avg length: 414.0878243512974\n",
            "Total length: 414916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIUv-STsYNJO",
        "outputId": "a50e8321-ead8-47eb-a0c7-52f3de339f0d"
      },
      "source": [
        "len(vectorized_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8EIC5h5YaRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822bba07-7c79-444c-9f84-5671655138c5"
      },
      "source": [
        "414916/1002"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414.0878243512974"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU2mIzrocCgI"
      },
      "source": [
        "def avg_len(data):\r\n",
        "  total_len = 0\r\n",
        "  for sample in data:\r\n",
        "    total_len = total_len + len(sample[1])\r\n",
        "  return total_len/len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iMKzFqrkQKk",
        "outputId": "f77362f4-ea50-4d02-f5db-0526e4ce0047"
      },
      "source": [
        "avg_len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1227.0439121756488"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Vn4yEbkR2J"
      },
      "source": [
        "def clean_data(data):\r\n",
        "  \"\"\"Shift to lower case, replace unknowns with UNK, and listify\"\"\"\r\n",
        "  new_data = []\r\n",
        "  VALID = 'abcdefghijklmnopqrstuvwxyz123456789\"\\'?!.,:; '\r\n",
        "  for sample in data:\r\n",
        "    new_sample = []\r\n",
        "    for char in sample[1].lower(): # Just grab the string, not the label\r\n",
        "       if char in VALID:\r\n",
        "         new_sample.append(char)\r\n",
        "       else:\r\n",
        "         new_sample.append('UNK')\r\n",
        "\r\n",
        "    new_data.append(new_sample)\r\n",
        "\r\n",
        "  return new_data\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu--zJ5hniFG"
      },
      "source": [
        "listified_data = clean_data(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utWt2gz8n_ey"
      },
      "source": [
        "def char_pad_trunc(data, maxlen=1500):\r\n",
        "  \"\"\"We truncate to maxlen or add in PAD tokens\"\"\"\r\n",
        "  new_dataset = []\r\n",
        "  for sample in data:\r\n",
        "    if len(sample) > maxlen:\r\n",
        "      new_data = sample[:maxlen]\r\n",
        "    elif len(sample) < maxlen:\r\n",
        "      pads = maxlen - len(sample)\r\n",
        "      new_data = sample + ['PAD'] * pads\r\n",
        "    else:\r\n",
        "      new_data = sample\r\n",
        "    new_dataset.append(new_data)\r\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cJ7-JKcpsPD"
      },
      "source": [
        "def create_dicts(data):\r\n",
        "  \"\"\"Modified from Keras LSTM example\"\"\"\r\n",
        "  chars = set()\r\n",
        "  for sample in data:\r\n",
        "    chars.update(set(sample))\r\n",
        "  char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
        "  indices_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "  return char_indices, indices_char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5siuuE4zryho"
      },
      "source": [
        "import numpy as np \r\n",
        "\r\n",
        "def onehot_encode(dataset, char_indicies, maxlen=1500):\r\n",
        "  \r\n",
        "  \"\"\"\r\n",
        "    One hot encode the tokens\r\n",
        "  \r\n",
        "\r\n",
        "  Args:\r\n",
        "       dataset list of lists of tokens\r\n",
        "       char_indicies dictionary of {key=character, value=index to use encoding vector}\r\n",
        "       maxlen int Length of each sample\r\n",
        "  Return:\r\n",
        "      np array of shape (samples, tokens, encoding length)\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\r\n",
        "  for i, sentence in enumerate(dataset):\r\n",
        "    for t, char in enumerate(sentence):\r\n",
        "      X[i, t, char_indicies[char]] = 1\r\n",
        "\r\n",
        "  return X\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icD-HMU0uEIL"
      },
      "source": [
        "maxlen = 1500\r\n",
        "common_length_data = char_pad_trunc(listified_data, maxlen)\r\n",
        "char_indices, indices_char = create_dicts(common_length_data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW8dioHuu5Bm",
        "outputId": "cfd0c0c8-66e1-4c8b-d8b1-cb824178c5a1"
      },
      "source": [
        "char_indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 37,\n",
              " '!': 35,\n",
              " '\"': 30,\n",
              " \"'\": 25,\n",
              " ',': 17,\n",
              " '.': 20,\n",
              " '1': 19,\n",
              " '2': 9,\n",
              " '3': 8,\n",
              " '4': 36,\n",
              " '5': 41,\n",
              " '6': 18,\n",
              " '7': 6,\n",
              " '8': 1,\n",
              " '9': 21,\n",
              " ':': 15,\n",
              " ';': 31,\n",
              " '?': 42,\n",
              " 'PAD': 5,\n",
              " 'UNK': 32,\n",
              " 'a': 22,\n",
              " 'b': 29,\n",
              " 'c': 23,\n",
              " 'd': 14,\n",
              " 'e': 28,\n",
              " 'f': 2,\n",
              " 'g': 4,\n",
              " 'h': 16,\n",
              " 'i': 24,\n",
              " 'j': 10,\n",
              " 'k': 7,\n",
              " 'l': 0,\n",
              " 'm': 26,\n",
              " 'n': 43,\n",
              " 'o': 40,\n",
              " 'p': 11,\n",
              " 'q': 12,\n",
              " 'r': 38,\n",
              " 's': 34,\n",
              " 't': 45,\n",
              " 'u': 3,\n",
              " 'v': 27,\n",
              " 'w': 33,\n",
              " 'x': 44,\n",
              " 'y': 13,\n",
              " 'z': 39}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lojy4xhfvRdF",
        "outputId": "22917d82-ac52-46ae-bb67-ffd3cab87a00"
      },
      "source": [
        "indices_char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'l',\n",
              " 1: '8',\n",
              " 2: 'f',\n",
              " 3: 'u',\n",
              " 4: 'g',\n",
              " 5: 'PAD',\n",
              " 6: '7',\n",
              " 7: 'k',\n",
              " 8: '3',\n",
              " 9: '2',\n",
              " 10: 'j',\n",
              " 11: 'p',\n",
              " 12: 'q',\n",
              " 13: 'y',\n",
              " 14: 'd',\n",
              " 15: ':',\n",
              " 16: 'h',\n",
              " 17: ',',\n",
              " 18: '6',\n",
              " 19: '1',\n",
              " 20: '.',\n",
              " 21: '9',\n",
              " 22: 'a',\n",
              " 23: 'c',\n",
              " 24: 'i',\n",
              " 25: \"'\",\n",
              " 26: 'm',\n",
              " 27: 'v',\n",
              " 28: 'e',\n",
              " 29: 'b',\n",
              " 30: '\"',\n",
              " 31: ';',\n",
              " 32: 'UNK',\n",
              " 33: 'w',\n",
              " 34: 's',\n",
              " 35: '!',\n",
              " 36: '4',\n",
              " 37: ' ',\n",
              " 38: 'r',\n",
              " 39: 'z',\n",
              " 40: 'o',\n",
              " 41: '5',\n",
              " 42: '?',\n",
              " 43: 'n',\n",
              " 44: 'x',\n",
              " 45: 't'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppi62NWpvrFE"
      },
      "source": [
        "encoded_data = onehot_encode(common_length_data, char_indices, maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44cwjydQvyEk",
        "outputId": "d2ea9d3c-0a6d-4ec1-fa19-1736829b99fb"
      },
      "source": [
        "len(listified_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLbYc-UXxTND",
        "outputId": "dff56ae6-5085-4276-a641-210310246aa4"
      },
      "source": [
        "encoded_data[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5zTNeMAxwjB"
      },
      "source": [
        "split_point = int(len(encoded_data) * .8)\r\n",
        "\r\n",
        "X_train = encoded_data[:split_point]\r\n",
        "y_train = expected[:split_point]\r\n",
        "X_test = encoded_data[split_point:]\r\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5oTIvq93cvb"
      },
      "source": [
        "X_train = np.array(X_train)\r\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atni6nV54XnK"
      },
      "source": [
        "X_test = np.array(X_test)\r\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJDxFvQu0jym",
        "outputId": "f4dac8d4-19f1-4f6c-d778-da5b615a7970"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhfptGAs0mPc",
        "outputId": "d3d798a1-a053-488a-90e7-69aae75a6404"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWuCVcVh0nfA"
      },
      "source": [
        "from keras.models import Sequential \r\n",
        "from keras.layers import Dense, Dropout, Embedding, Flatten, LSTM "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE10OkUH1Aoc",
        "outputId": "ebf561f6-424f-4c62-df66-d28aa8b1cc71"
      },
      "source": [
        "num_neurons = 40\r\n",
        "maxlen = 1500\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, len(char_indices.keys()))))\r\n",
        "model.add(Dropout(.2))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 1500, 40)          13920     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1500, 40)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 60000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 60001     \n",
            "=================================================================\n",
            "Total params: 73,921\n",
            "Trainable params: 73,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTweJaJB10xe",
        "outputId": "e9c914c4-1520-4187-e03f-df69bea59161"
      },
      "source": [
        "batch_size = 32\r\n",
        "epochs = 10\r\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "26/26 [==============================] - 15s 576ms/step - loss: 0.8904 - accuracy: 0.4881 - val_loss: 0.9857 - val_accuracy: 0.5274\n",
            "Epoch 2/10\n",
            "26/26 [==============================] - 17s 650ms/step - loss: 0.6888 - accuracy: 0.6442 - val_loss: 0.7838 - val_accuracy: 0.5025\n",
            "Epoch 3/10\n",
            "26/26 [==============================] - 17s 649ms/step - loss: 0.5797 - accuracy: 0.7603 - val_loss: 1.1379 - val_accuracy: 0.5174\n",
            "Epoch 4/10\n",
            "26/26 [==============================] - 17s 651ms/step - loss: 0.5205 - accuracy: 0.8414 - val_loss: 0.7289 - val_accuracy: 0.5522\n",
            "Epoch 5/10\n",
            "26/26 [==============================] - 17s 664ms/step - loss: 0.4125 - accuracy: 0.8702 - val_loss: 1.1307 - val_accuracy: 0.4925\n",
            "Epoch 6/10\n",
            "26/26 [==============================] - 17s 660ms/step - loss: 0.3561 - accuracy: 0.9114 - val_loss: 0.9755 - val_accuracy: 0.5224\n",
            "Epoch 7/10\n",
            "26/26 [==============================] - 17s 671ms/step - loss: 0.2719 - accuracy: 0.9376 - val_loss: 0.8246 - val_accuracy: 0.5423\n",
            "Epoch 8/10\n",
            "26/26 [==============================] - 18s 710ms/step - loss: 0.2185 - accuracy: 0.9613 - val_loss: 0.9299 - val_accuracy: 0.5224\n",
            "Epoch 9/10\n",
            "26/26 [==============================] - 17s 652ms/step - loss: 0.1734 - accuracy: 0.9675 - val_loss: 0.9101 - val_accuracy: 0.5423\n",
            "Epoch 10/10\n",
            "26/26 [==============================] - 17s 653ms/step - loss: 0.1228 - accuracy: 0.9875 - val_loss: 0.9716 - val_accuracy: 0.4925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3233338b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsB5WVJw2XXi"
      },
      "source": [
        "model_structure = model.to_json()\r\n",
        "with open(\"char_lstm_model3.json\", \"w\") as json_file:\r\n",
        "  json_file.write(model_structure)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeHwY_TG59K2"
      },
      "source": [
        "model.save_weights(\"char_lstm_weights3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG0A80g56DSC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}